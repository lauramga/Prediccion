histogram=TRUE, pch=19)
# Empleamos la funcion lm() para generar un modelo de regresión lineal por
# minimos cuadrados en el que la variable de respuesta son los jugadores en el
# que la variable de respuesta es..... y el predictor.....
modelo_regresion_vif <- lm(salary~.-player-nba_country-tm, data=log_data)
vif_values <- car::vif(model_vif)
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "steelblue")
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
knitr::kable(vif_values)
## Model Selection
nba <- log_data %>% select_at(vars(-vars))
set.seed(1234)
num_data <- nrow(nba)
num_data_test <- 10
train=sample(num_data ,num_data-num_data_test)
data_train <- nba[train,]
data_test  <-  nba[-train,]
model_select <- regsubsets(salary~. , data =data_train, method = "seqrep",nvmax=24)
model_select_summary <- summary(model_select)
data.frame(
Adj.R2 = (model_select_summary$adjr2),
CP = (model_select_summary$cp),
BIC = (model_select_summary$bic)
)
model_select_summary$outmat
plot(model_select, scale = "bic", main = "BIC")
data.frame(
Adj.R2 = which.max(model_select_summary$adjr2),
CP = which.min(model_select_summary$cp),
BIC = which.min(model_select_summary$bic)
)
coef(model_select,which.min(model_select_summary$adjr2))
coef(model_select,which.min(model_select_summary$cp))
coef(model_select,which.min(model_select_summary$bic))
# adjR2 model
nba_r2 <- lm(salary~ mp , data =data_train)
summary(nba_r2)
# CP model
nba_cp <- lm(salary~ nba_draft_number+age+mp+per+ts+f_tr+trb+ast+tov+usg+dws+ws_48+dbpm, data =data_train)
summary(nba_cp)
# BIC model
nba_bic <- lm(salary~ nba_draft_number+age+mp+drb, data =data_train)
summary(nba_bic)
# Prediction
# adjR2
predict_r2 <- predict(nba_r2,newdata = data_test)
exp(cbind(predict_r2,data_test$salary))
mean((data_test$salary-predict_r2)^2)
sqrt(mean((data_test$salary-predict_r2)^2))
# CP
predict_cp <- predict(nba_cp,newdata = data_test)
cbind(predict_cp,data_test$salary)
exp(cbind(predict_cp,data_test$salary))
mean((data_test$salary-predict_cp)^2)
sqrt(mean((data_test$salary-predict_cp)^2))
# BIC (Bayesiano)
predict_bic <- predict(nba_bic,newdata = data_test)
cbind(predict_bic,data_test$salary)
exp(cbind(predict_bic,data_test$salary))
mean((data_test$salary-predict_bic)^2)
sqrt(mean((data_test$salary-predict_bic)^2))
library(tidyverse)
library(skimr) # == Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
library(caret) # Cross Validation
library(bestglm) # Cross Validation
library(glmnet) # Regularization
datos_nba <- read_csv("nba.csv")
datos_nba %<>% distinct(Player,.keep_all= TRUE)
datos_nba %<>% drop_na()
skim(datos_nba)
colnames(datos_nba)
# Correlations
corrplot(cor(datos_nba %>%
select_at(vars(-Player, -NBA_Country, -Tm)),
use = "complete.obs"),
method = "circle",type = "upper")
# Las estrellas en rojo lo que indica es que si es distinta de 0, es decir cuales estan correlacionadas.
# Las lineas rectas determinan que no hay relacion entre ellas
chart.Correlation(datos_nba %>%
select_at(vars(-Player, -NBA_Country, -Tm)),
histogram=TRUE, pch=19)
log_datos_nba <- datos_nba %>%
mutate(Salary=log(Salary))
skim(log_datos_nba)
chart.Correlation(log_datos_nba %>%
select_at(vars(-Player, -NBA_Country, -Tm)),
histogram=TRUE, pch=19)
nba <- datos_nba %>% select_at(vars(-Player, -NBA_Country, -Tm))
modelo <- regsubsets(Salary~. , data =nba, method = "seqrep",nvmax=24)
summary(modelo)
plot(modelo, scale = "adjr2", main = "Adjusted R^2")
# get_model_formula(), allowing to access easily the formula of the models returned by the function regsubsets(). Copy and paste the following code in your R console:
# id: model id
# object: regsubsets object
# data: data used to fit regsubsets
# outcome: outcome variable
get_model_formula <- function(id, object, outcome){
# get models data
models <- summary(object)$which[id,-1]
# Get model predictors
predictors <- names(which(models == TRUE))
predictors <- paste(predictors, collapse = "+")
# Build model formula
as.formula(paste0(outcome, "~", predictors))
}
# get_cv_error(), to get the cross-validation (CV) error for a given model:
#En la semilla y el kfold lo pone por defecto con un valor en la funcion.
get_cv_error <- function(model.formula, data, kfold=5, setseed=1){
set.seed(setseed)
train.control <- trainControl(method = "cv",
number = kfold)
cv <- train(model.formula,
data = data, method = "lm",
trControl = train.control)
cv$results$RMSE
}
# Compute cross-validation error
model.ids <- c(8,7,5)
#La funcion MAP evita hacer bucles. Coge esto, introducelo en la formula con la variable salary, este resultado me lo introduces en map, lo siguiente y asi.
cv.errors <-  map(model.ids, get_model_formula,model_select , "Salary") %>%
map(get_cv_error, data = nba,kfold=10,setseed=12345) %>%
unlist()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
datos_nba <- read_csv("nba.csv")
datos_nba %<>% distinct(Player,.keep_all= TRUE)
datos_nba %<>% drop_na()
skim(datos_nba)
colnames(datos_nba)
# Correlations
corrplot(cor(datos_nba %>%
select_at(vars(-Player, -NBA_Country, -Tm)),
use = "complete.obs"),
method = "circle",type = "upper")
# Las estrellas en rojo lo que indica es que si es distinta de 0, es decir cuales estan correlacionadas.
# Las lineas rectas determinan que no hay relacion entre ellas
chart.Correlation(datos_nba %>%
select_at(vars(-Player, -NBA_Country, -Tm)),
histogram=TRUE, pch=19)
log_datos_nba <- datos_nba %>%
mutate(Salary=log(Salary))
skim(log_datos_nba)
chart.Correlation(log_datos_nba %>%
select_at(vars(-Player, -NBA_Country, -Tm)),
histogram=TRUE, pch=19)
nba <- datos_nba %>% select_at(vars(-Player, -NBA_Country, -Tm))
# library(rsample)
set.seed(123)
nba_split <- initial_split(nba, prop= 0.80, strata = "Salary")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(skimr) # == Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
library(caret) # Cross Validation
library(bestglm) # Cross Validation
library(glmnet) # Regularization
library(rsample) # para cv split
library(boot)
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(skimr) # == Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
library(caret) # Cross Validation
library(bestglm) # Cross Validation
library(glmnet) # Regularization
library(rsample) # para cv split
library(boot)
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)
library(tidyverse)
library(skimr) # == Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
library(caret) # Cross Validation
library(bestglm) # Cross Validation
library(glmnet) # Regularization
library(rsample) # para cv split
library(boot)
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)
datos_nba <- read_csv("nba.csv")
datos_nba %<>% distinct(Player,.keep_all= TRUE)
datos_nba %<>% drop_na()
skim(datos_nba)
colnames(datos_nba)
# Correlations
corrplot(cor(datos_nba %>%
select_at(vars(-Player, -NBA_Country, -Tm)),
use = "complete.obs"),
method = "circle",type = "upper")
# Las estrellas en rojo lo que indica es que si es distinta de 0, es decir cuales estan correlacionadas.
# Las lineas rectas determinan que no hay relacion entre ellas
chart.Correlation(datos_nba %>%
select_at(vars(-Player, -NBA_Country, -Tm)),
histogram=TRUE, pch=19)
log_datos_nba <- datos_nba %>%
mutate(Salary=log(Salary))
skim(log_datos_nba)
chart.Correlation(log_datos_nba %>%
select_at(vars(-Player, -NBA_Country, -Tm)),
histogram=TRUE, pch=19)
nba <- datos_nba %>% select_at(vars(-Player, -NBA_Country, -Tm))
# library(rsample)
set.seed(123)
nba_split <- initial_split(nba, prop= 0.80, strata = "Salary")
nba_train <- training(nba_split)
nba_test <- testing(nba_split)
regres_train <- lm(nba, nba_train )
regres_train1 <- lm(nba, nba_train )
c(AIC(regres_train),AIC(regres_train1))
pred_0 <- predict(regres_train, newdata = nba_test)
MSE0 <- mean((data_test$Salary-pred_0)^2)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
pred_0 <- predict(regres_train, newdata = nba_test)
MSE0 <- mean((nba_test$Salary-pred_0)^2)
pred_1 <- predict(regres_train1,newdata = nba_test)
MSE1 <- mean((nba_test$Salary-pred_1)^2)
c(MSE0,MSE1)
# library(glmnet)
# library (boot)
set.seed(123)
glm.fit1=glm(nba,datos_nba,family = gaussian())
coef(glm.fit1)
cv.err =cv.glm(datos_nba,glm.fit1)
cv.err$delta
glm.fit2=glm(nba, datos_nba,family = gaussian())
cv.err2 = cv.glm(datos_nba,glm.fit2)
cv.err2$delta
set.seed(123)
cv.err =cv.glm(datos_nba,glm.fit1,K=10)
cv.err$delta
glm.fit2=glm(nba, datos_nba, family = gaussian())
cv.err2 =cv.glm(datos_nba,glm.fit2,K=10)
cv.err2$delta
# library(glmnet)
# library(dplyr)
# library(ggplot2)
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Salary")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# library(glmnet)
# library(dplyr)
# library(ggplot2)
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(nbz, prop = .7, strata = "Salary")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# library(glmnet)
# library(dplyr)
# library(ggplot2)
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(nba, prop = .7, strata = "Salary")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
ames_train_x <- model.matrix(Salary~ ., ames_train)[, -1]
ames_train_y <- log(ames_train$Sale_Price)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
ames_train_x <- model.matrix(Salary~ ., ames_train)[, -1]
ames_train_y <- log(ames_train$Salary)
ames_test_x <- model.matrix(Salary~ ., ames_test)[, -1]
ames_test_y <- log(ames_test$Salary)
# Ridge regression
ames_ridge <- glmnet(
x = ames_train_x,
y = ames_train_y,
alpha = 0
)
plot(ames_ridge, xvar = "lambda")
ames_ridge$lambda %>% head()
# coefficients for the largest and smallest lambda parameters
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 100]
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# CV Ridge regression to ames data
ames_ridge_cv <- cv.glmnet(
x = ames_train_x,
y = ames_train_y,
alpha = 0
)
plot(ames_ridge_cv)
min(ames_ridge_cv$cvm)       # minimum MSE
ames_ridge_cv$lambda.min     # lambda for this min MSE
log(ames_ridge_cv$lambda.min)
ames_ridge_cv$cvm[ames_ridge_cv$lambda == ames_ridge_cv$lambda.1se]  # 1 st.error of min MSE
ames_ridge_cv$lambda.1se  # lambda for this MSE
plot(ames_ridge, xvar = "lambda")
abline(v = log(ames_ridge_cv$lambda.1se), col = "red", lty = "dashed")
# lasso regression to ames data: alpha=1
ames_lasso <- glmnet(
x = ames_train_x,
y = ames_train_y,
alpha = 1
)
plot(ames_lasso, xvar = "lambda")
# CV Ridge regression
ames_lasso_cv <- cv.glmnet(
x = ames_train_x,
y = ames_train_y,
alpha = 1
)
plot(ames_lasso_cv)
min(ames_lasso_cv$cvm)       # minimum MSE
ames_lasso_cv$lambda.min     # lambda for this min MSE
ames_lasso_cv$lambda.1se  # lambda for this MSE
plot(ames_lasso, xvar = "lambda")
abline(v = log(ames_lasso_cv$lambda.min), col = "red", lty = "dashed")
abline(v = log(ames_lasso_cv$lambda.1se), col = "red", lty = "dashed")
coef(ames_lasso_cv, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
min(ames_ridge_cv$cvm) # minimum Ridge MSE
min(ames_lasso_cv$cvm) # minimum Lasso MSE
lasso    <- glmnet(ames_train_x, ames_train_y, alpha = 1.0)
elastic1 <- glmnet(ames_train_x, ames_train_y, alpha = 0.25)
elastic2 <- glmnet(ames_train_x, ames_train_y, alpha = 0.75)
ridge    <- glmnet(ames_train_x, ames_train_y, alpha = 0.0)
par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
fold_id <- sample(1:10, size = length(ames_train_y), replace=TRUE)
tuning_grid <- tibble::tibble(
alpha      = seq(0, 1, by = .1),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
tuning_grid
for(i in seq_along(tuning_grid$alpha)) {
# fit CV model for each alpha value
fit <- cv.glmnet(ames_train_x, ames_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
# extract MSE and lambda values
tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
tuning_grid$lambda_min[i] <- fit$lambda.min
tuning_grid$lambda_1se[i] <- fit$lambda.1se
}
tuning_grid
tuning_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
ggtitle("MSE ± one standard error")
cv_lasso   <- cv.glmnet(ames_train_x, ames_train_y, alpha = 1.0)
min(cv_lasso$cvm)
pred <- predict(cv_lasso, s = cv_lasso$lambda.min, ames_test_x)
mean((ames_test_y - pred)^2)
# best model
cv_net   <- cv.glmnet(ames_train_x, ames_train_y, alpha = 0.2)
min(cv_net$cvm)
# predict
pred <- predict(cv_net, s = cv_net$lambda.min, ames_test_x)
mean((ames_test_y - pred)^2)
# Libraries and functions
library(readr)
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
# Read Data
DATOS_NBA <- read.csv("nba.csv", sep= ",",  fileEncoding="latin1")
View(DATOS_NBA)
# Variables Names
DATOS_NBA %<>% clean_names()
colnames(DATOS_NBA)
# Summarize Data
skim(DATOS_NBA)
# Data Wrangling data: process of cleaning and unifying complex data sets for
# analysis, in turn boosting productivity within an organization.
# delete duplicate
# Remove duplicate rows of the dataframe
DATOS_NBA %<>% distinct(player,.keep_all= TRUE)
# delete NA's
DATOS_NBA %<>% drop_na()
# Summarise
skim(DATOS_NBA)
DATOS_NBA %>%
select_at(vars(-c("player","nba_country","tm"))) %>%
tidyr::gather("id", "value", 2:25) %>%
ggplot(., aes(y=salary, x=value))+
geom_point()+
geom_smooth(method = "lm", se=FALSE, color="black")+
facet_wrap(~id,ncol=2,scales="free_x")
DATOS_NBA %>%
select_at(vars(-c("player","nba_country","tm"))) %>%
tidyr::gather("id", "value", 2:25) %>%
ggplot(., aes(y=log(salary), x=value))+
geom_point()+
geom_smooth(method = "lm", se=FALSE, color="black")+
facet_wrap(~id,ncol=2,scales="free_x")
## EDA
# Log salary
log_data <- DATOS_NBA %>% mutate(salary=log(salary))
skim(log_data)
# Excluded vars (factor)
vars <- c("player","nba_country","tm")
# Correlations
corrplot(cor(log_data %>%
select_at(vars(-vars)),
use = "complete.obs"),
method = "circle",type = "upper")
# Other Correlations
ggcorrplot(cor(log_data %>%
select_at(vars(-vars)),
use = "complete.obs"),
hc.order = TRUE,
type = "lower",  lab = TRUE)
# Other Correlations
chart.Correlation(log_data %>%
select_at(vars(-vars)),
histogram=TRUE, pch=19)
# Empleamos la funcion lm() para generar un modelo de regresión lineal por
# minimos cuadrados en el que la variable de respuesta son los jugadores en el
# que la variable de respuesta es..... y el predictor.....
modelo_regresion_vif <- lm(salary~.-player-nba_country-tm, data=log_data)
vif_values <- car::vif(model_vif)
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "steelblue")
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
knitr::kable(vif_values)
## Model Selection
nba <- log_data %>% select_at(vars(-vars))
set.seed(1234)
num_data <- nrow(nba)
num_data_test <- 10
train=sample(num_data ,num_data-num_data_test)
data_train <- nba[train,]
data_test  <-  nba[-train,]
model_select <- regsubsets(salary~. , data =data_train, method = "seqrep",nvmax=24)
model_select_summary <- summary(model_select)
data.frame(
Adj.R2 = (model_select_summary$adjr2),
CP = (model_select_summary$cp),
BIC = (model_select_summary$bic)
)
model_select_summary$outmat
plot(model_select, scale = "bic", main = "BIC")
data.frame(
Adj.R2 = which.max(model_select_summary$adjr2),
CP = which.min(model_select_summary$cp),
BIC = which.min(model_select_summary$bic)
)
coef(model_select,which.min(model_select_summary$adjr2))
coef(model_select,which.min(model_select_summary$cp))
coef(model_select,which.min(model_select_summary$bic))
# adjR2 model
nba_r2 <- lm(salary~ mp , data =data_train)
summary(nba_r2)
# CP model
nba_cp <- lm(salary~ nba_draft_number+age+mp+per+ts+f_tr+trb+ast+tov+usg+dws+ws_48+dbpm, data =data_train)
summary(nba_cp)
# BIC model
nba_bic <- lm(salary~ nba_draft_number+age+mp+drb, data =data_train)
summary(nba_bic)
# Prediction
# adjR2
predict_r2 <- predict(nba_r2,newdata = data_test)
exp(cbind(predict_r2,data_test$salary))
mean((data_test$salary-predict_r2)^2)
sqrt(mean((data_test$salary-predict_r2)^2))
# CP
predict_cp <- predict(nba_cp,newdata = data_test)
cbind(predict_cp,data_test$salary)
exp(cbind(predict_cp,data_test$salary))
mean((data_test$salary-predict_cp)^2)
sqrt(mean((data_test$salary-predict_cp)^2))
# BIC (Bayesiano)
predict_bic <- predict(nba_bic,newdata = data_test)
cbind(predict_bic,data_test$salary)
exp(cbind(predict_bic,data_test$salary))
mean((data_test$salary-predict_bic)^2)
sqrt(mean((data_test$salary-predict_bic)^2))
